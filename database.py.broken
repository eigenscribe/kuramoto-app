"""
Database module for the Kuramoto Model Simulator.
This module provides functionality to store and retrieve simulation data,
as well as import/export configurations as JSON files.

Enhanced with machine learning support for:
- Dataset generation and export
- Feature extraction for neural networks
- Batch simulation processing
- Time series analysis
"""

import os
import numpy as np
import json
import pickle
import pandas as pd
from datetime import datetime
from sqlalchemy import create_engine, Column, Integer, Float, String, DateTime, LargeBinary, ForeignKey, Text, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship

# Create the engine and base
DATABASE_URL = "sqlite:///kuramoto_simulations.db"
engine = create_engine(DATABASE_URL)
Base = declarative_base()

class Simulation(Base):
    """Model representing a Kuramoto simulation run."""
    __tablename__ = "simulations"

    id = Column(Integer, primary_key=True)
    timestamp = Column(DateTime, default=datetime.now)
    n_oscillators = Column(Integer)
    coupling_strength = Column(Float)
    simulation_time = Column(Float)
    time_step = Column(Float)
    random_seed = Column(Integer)
    frequency_distribution = Column(String(50))
    frequency_params = Column(Text)  # JSON string of distribution parameters
    
    # Relationships
    frequencies = relationship("Frequency", back_populates="simulation", cascade="all, delete-orphan")
    phases = relationship("Phase", back_populates="simulation", cascade="all, delete-orphan")
    order_parameters = relationship("OrderParameter", back_populates="simulation", cascade="all, delete-orphan")
    adjacency_matrix = relationship("AdjacencyMatrix", back_populates="simulation", uselist=False, cascade="all, delete-orphan")

    def __repr__(self):
        return f"<Simulation(id={self.id}, oscillators={self.n_oscillators}, coupling={self.coupling_strength})>"

class Frequency(Base):
    """Model representing oscillator natural frequencies."""
    __tablename__ = "frequencies"

    id = Column(Integer, primary_key=True)
    simulation_id = Column(Integer, ForeignKey("simulations.id"))
    oscillator_index = Column(Integer)
    value = Column(Float)
    
    # Relationship back to simulation
    simulation = relationship("Simulation", back_populates="frequencies")

    def __repr__(self):
        return f"<Frequency(oscillator={self.oscillator_index}, value={self.value})>"

class Phase(Base):
    """Model representing phase data for each oscillator over time."""
    __tablename__ = "phases"

    id = Column(Integer, primary_key=True)
    simulation_id = Column(Integer, ForeignKey("simulations.id"))
    time_index = Column(Integer)
    oscillator_index = Column(Integer)
    value = Column(Float)
    
    # Relationship back to simulation
    simulation = relationship("Simulation", back_populates="phases")

    def __repr__(self):
        return f"<Phase(time={self.time_index}, oscillator={self.oscillator_index}, value={self.value})>"

class OrderParameter(Base):
    """Model representing order parameter data over time."""
    __tablename__ = "order_parameters"

    id = Column(Integer, primary_key=True)
    simulation_id = Column(Integer, ForeignKey("simulations.id"))
    time_index = Column(Integer)
    magnitude = Column(Float)
    phase = Column(Float)
    
    # Relationship back to simulation
    simulation = relationship("Simulation", back_populates="order_parameters")

    def __repr__(self):
        return f"<OrderParameter(time={self.time_index}, r={self.magnitude}, psi={self.phase})>"

class AdjacencyMatrix(Base):
    """Model representing the network adjacency matrix."""
    __tablename__ = "adjacency_matrices"

    id = Column(Integer, primary_key=True)
    simulation_id = Column(Integer, ForeignKey("simulations.id"), unique=True)
    data = Column(LargeBinary)  # Stored as a pickled numpy array
    
    # Relationship back to simulation
    simulation = relationship("Simulation", back_populates="adjacency_matrix")

    def __repr__(self):
        return f"<AdjacencyMatrix(simulation_id={self.simulation_id})>"

class Configuration(Base):
    """Model representing a saved simulation configuration."""
    __tablename__ = "configurations"
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False, unique=True)
    timestamp = Column(DateTime, default=datetime.now)
    n_oscillators = Column(Integer)
    coupling_strength = Column(Float)
    simulation_time = Column(Float)
    time_step = Column(Float)
    random_seed = Column(Integer)
    network_type = Column(String(50))
    frequency_distribution = Column(String(50))
    frequency_params = Column(Text)  # JSON string of distribution parameters
    adjacency_matrix = Column(LargeBinary, nullable=True)  # For custom adjacency matrices
    
    def __repr__(self):
        return f"<Configuration(id={self.id}, name='{self.name}')>"


class MLDataset(Base):
    """Model representing a ML-ready dataset composed of multiple simulations."""
    __tablename__ = "ml_datasets"
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False, unique=True)
    timestamp = Column(DateTime, default=datetime.now)
    description = Column(Text, nullable=True)
    feature_type = Column(String(50))  # e.g., 'time_series', 'graph', 'spectral'
    target_type = Column(String(50))   # e.g., 'classification', 'regression', 'forecasting'
    preprocessing = Column(Text)      # JSON string of preprocessing steps
    
    # Relationships
    simulations = relationship("MLDatasetSimulation", back_populates="dataset", cascade="all, delete-orphan")
    features = relationship("MLFeature", back_populates="dataset", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<MLDataset(id={self.id}, name='{self.name}', type='{self.feature_type}')>"


class MLDatasetSimulation(Base):
    """Model representing a simulation included in an ML dataset."""
    __tablename__ = "ml_dataset_simulations"
    
    id = Column(Integer, primary_key=True)
    dataset_id = Column(Integer, ForeignKey("ml_datasets.id"))
    simulation_id = Column(Integer, ForeignKey("simulations.id"))
    split = Column(String(20))  # 'train', 'validation', 'test'
    
    # Relationships
    dataset = relationship("MLDataset", back_populates="simulations")
    
    def __repr__(self):
        return f"<MLDatasetSimulation(dataset_id={self.dataset_id}, simulation_id={self.simulation_id}, split='{self.split}')>"


class MLFeature(Base):
    """Model representing features extracted from simulations for ML."""
    __tablename__ = "ml_features"
    
    id = Column(Integer, primary_key=True)
    dataset_id = Column(Integer, ForeignKey("ml_datasets.id"))
    name = Column(String(100))
    description = Column(Text, nullable=True)
    data = Column(LargeBinary)  # Pickled numpy array or pandas dataframe
    feature_type = Column(String(50))  # 'input', 'target', 'metadata'
    
    # Relationships
    dataset = relationship("MLDataset", back_populates="features")
    
    def __repr__(self):
        return f"<MLFeature(id={self.id}, name='{self.name}', type='{self.feature_type}')>"

# Create all tables
Base.metadata.create_all(engine)

# Create a session factory
Session = sessionmaker(bind=engine)

def store_simulation(model, times, phases, order_parameter, frequencies, freq_type, freq_params=None, adjacency_matrix=None):
    """
    Store simulation data in the database.
    
    Parameters:
    -----------
    model : KuramotoModel
        The model object containing simulation parameters
    times : ndarray
        Time points of the simulation
    phases : ndarray
        Phases of oscillators at each time point
    order_parameter : ndarray
        Order parameter r(t) at each time point
    frequencies : ndarray
        Natural frequencies of the oscillators
    freq_type : str
        Type of frequency distribution (e.g., 'Normal', 'Uniform')
    freq_params : dict, optional
        Parameters of the frequency distribution
    adjacency_matrix : ndarray, optional
        Adjacency matrix representing network structure
        
    Returns:
    --------
    int
        The ID of the stored simulation
    """
    session = Session()
    
    # Create new simulation record
    sim = Simulation(
        n_oscillators=model.n_oscillators,
        coupling_strength=model.coupling_strength,
        simulation_time=model.simulation_time,
        time_step=model.time_step,
        random_seed=model.random_seed,
        frequency_distribution=freq_type,
        frequency_params=json.dumps(freq_params) if freq_params else None
    )
    
    session.add(sim)
    session.flush()  # Get the simulation ID without committing
    
    # Store natural frequencies
    for i, freq in enumerate(frequencies):
        session.add(Frequency(
            simulation_id=sim.id,
            oscillator_index=i,
            value=float(freq)
        ))
    
    # Store phases at selected time points (we can't store all time points for large simulations)
    # Take ~100 time points or all if there are fewer
    time_indices = np.linspace(0, len(times)-1, min(100, len(times)), dtype=int)
    for t_idx in time_indices:
        for i in range(model.n_oscillators):
            session.add(Phase(
                simulation_id=sim.id,
                time_index=t_idx,
                oscillator_index=i,
                value=float(phases[i, t_idx])
            ))
    
    # Store order parameter
    psi = np.angle(np.sum(np.exp(1j * phases), axis=0))
    for t_idx in time_indices:
        session.add(OrderParameter(
            simulation_id=sim.id,
            time_index=t_idx,
            magnitude=float(order_parameter[t_idx]),
            phase=float(psi[t_idx])
        ))
    
    # Store adjacency matrix if provided
    if adjacency_matrix is not None:
        session.add(AdjacencyMatrix(
            simulation_id=sim.id,
            data=adjacency_matrix.tobytes()
        ))
    
    session.commit()
    sim_id = sim.id
    session.close()
    
    return sim_id

def get_simulation(simulation_id):
    """
    Retrieve a simulation from the database.
    
    Parameters:
    -----------
    simulation_id : int
        The ID of the simulation to retrieve
        
    Returns:
    --------
    dict
        Dictionary containing simulation data
    """
    session = Session()
    
    sim = session.query(Simulation).filter_by(id=simulation_id).first()
    if not sim:
        session.close()
        return None
    
    # Get frequencies
    freqs = session.query(Frequency).filter_by(simulation_id=simulation_id).all()
    frequencies = np.array([f.value for f in sorted(freqs, key=lambda x: x.oscillator_index)])
    
    # Get phases
    phases_data = session.query(Phase).filter_by(simulation_id=simulation_id).all()
    time_indices = sorted(list(set([p.time_index for p in phases_data])))
    n_oscillators = sim.n_oscillators
    
    phases = np.zeros((n_oscillators, len(time_indices)))
    for phase in phases_data:
        t_idx = time_indices.index(phase.time_index)
        phases[phase.oscillator_index, t_idx] = phase.value
    
    # Get order parameters
    order_params = session.query(OrderParameter).filter_by(simulation_id=simulation_id).all()
    order_params.sort(key=lambda x: x.time_index)
    
    r = np.array([op.magnitude for op in order_params])
    psi = np.array([op.phase for op in order_params])
    
    # Get adjacency matrix if it exists
    adj_matrix = session.query(AdjacencyMatrix).filter_by(simulation_id=simulation_id).first()
    adjacency_matrix = None
    if adj_matrix:
        adjacency_matrix = np.frombuffer(adj_matrix.data).reshape((n_oscillators, n_oscillators))
    
    # Create the result dictionary
    result = {
        'id': sim.id,
        'timestamp': sim.timestamp,
        'n_oscillators': sim.n_oscillators,
        'coupling_strength': sim.coupling_strength,
        'simulation_time': sim.simulation_time,
        'time_step': sim.time_step,
        'random_seed': sim.random_seed,
        'frequency_distribution': sim.frequency_distribution,
        'frequency_params': json.loads(sim.frequency_params) if sim.frequency_params else None,
        'frequencies': frequencies,
        'phases': phases,
        'time_indices': time_indices,
        'order_parameter': {
            'r': r,
            'psi': psi
        },
        'adjacency_matrix': adjacency_matrix
    }
    
    session.close()
    return result

def list_simulations():
    """
    List all simulations in the database.
    
    Returns:
    --------
    list
        List of dictionaries containing basic simulation info
    """
    session = Session()
    
    sims = session.query(Simulation).all()
    result = [{
        'id': sim.id,
        'timestamp': sim.timestamp,
        'n_oscillators': sim.n_oscillators,
        'coupling_strength': sim.coupling_strength,
        'frequency_distribution': sim.frequency_distribution
    } for sim in sims]
    
    session.close()
    return result

def delete_simulation(simulation_id):
    """
    Delete a simulation from the database.
    
    Parameters:
    -----------
    simulation_id : int
        The ID of the simulation to delete
        
    Returns:
    --------
    bool
        True if deletion was successful, False otherwise
    """
    session = Session()
    
    sim = session.query(Simulation).filter_by(id=simulation_id).first()
    if not sim:
        session.close()
        return False
    
    session.delete(sim)
    session.commit()
    session.close()
    
    return True

def save_configuration(name, n_oscillators, coupling_strength, simulation_time, time_step, 
                      random_seed, network_type, frequency_distribution, frequency_params,
                      adjacency_matrix=None):
    """
    Save a simulation configuration to the database.
    
    Parameters:
    -----------
    name : str
        Name to identify this configuration
    n_oscillators : int
        Number of oscillators
    coupling_strength : float
        Coupling strength
    simulation_time : float
        Total simulation time
    time_step : float
        Simulation time step
    random_seed : int
        Random seed for reproducibility
    network_type : str
        Type of network connectivity
    frequency_distribution : str
        Type of frequency distribution
    frequency_params : dict
        Parameters of the frequency distribution
    adjacency_matrix : ndarray, optional
        Custom adjacency matrix for network connectivity
        
    Returns:
    --------
    int
        The ID of the saved configuration, or None if there was an error
    """
    session = Session()
    
    # Check if a configuration with this name already exists
    existing = session.query(Configuration).filter_by(name=name).first()
    if existing:
        session.close()
        return None
    
    # Prepare the adjacency matrix data if provided
    adj_matrix_data = None
    if adjacency_matrix is not None:
        adj_matrix_data = adjacency_matrix.tobytes()
    
    # Create new configuration
    config = Configuration(
        name=name,
        n_oscillators=n_oscillators,
        coupling_strength=coupling_strength,
        simulation_time=simulation_time,
        time_step=time_step,
        random_seed=random_seed,
        network_type=network_type,
        frequency_distribution=frequency_distribution,
        frequency_params=json.dumps(frequency_params) if frequency_params else None,
        adjacency_matrix=adj_matrix_data
    )
    
    try:
        session.add(config)
        session.commit()
        config_id = config.id
        session.close()
        return config_id
    except Exception as e:
        session.rollback()
        session.close()
        raise e

def list_configurations():
    """
    List all saved configurations.
    
    Returns:
    --------
    list
        List of dictionaries containing configuration info
    """
    session = Session()
    
    configs = session.query(Configuration).all()
    result = [{
        'id': config.id,
        'name': config.name,
        'timestamp': config.timestamp,
        'n_oscillators': config.n_oscillators,
        'coupling_strength': config.coupling_strength,
        'network_type': config.network_type,
        'frequency_distribution': config.frequency_distribution
    } for config in configs]
    
    session.close()
    return result

def get_configuration(config_id):
    """
    Retrieve a configuration from the database.
    
    Parameters:
    -----------
    config_id : int
        The ID of the configuration to retrieve
        
    Returns:
    --------
    dict
        Dictionary containing configuration data
    """
    session = Session()
    
    config = session.query(Configuration).filter_by(id=config_id).first()
    if not config:
        session.close()
        return None
    
    # Process adjacency matrix if it exists
    adjacency_matrix = None
    if config.adjacency_matrix:
        try:
            adjacency_matrix = np.frombuffer(config.adjacency_matrix).reshape((config.n_oscillators, config.n_oscillators))
        except:
            # If there's an error reshaping, just leave as None
            pass
    
    # Create the result dictionary
    result = {
        'id': config.id,
        'name': config.name,
        'timestamp': config.timestamp,
        'n_oscillators': config.n_oscillators,
        'coupling_strength': config.coupling_strength,
        'simulation_time': config.simulation_time,
        'time_step': config.time_step,
        'random_seed': config.random_seed,
        'network_type': config.network_type,
        'frequency_distribution': config.frequency_distribution,
        'frequency_params': json.loads(config.frequency_params) if config.frequency_params else None,
        'adjacency_matrix': adjacency_matrix
    }
    
    session.close()
    return result

def delete_configuration(config_id):
    """
    Delete a configuration from the database.
    
    Parameters:
    -----------
    config_id : int
        The ID of the configuration to delete
        
    Returns:
    --------
    bool
        True if deletion was successful, False otherwise
    """
    session = Session()
    
    config = session.query(Configuration).filter_by(id=config_id).first()
    if not config:
        session.close()
        return False
    
    session.delete(config)
    session.commit()
    session.close()
    
    return True

def get_configuration_by_name(name):
    """
    Retrieve a configuration by name.
    
    Parameters:
    -----------
    name : str
        The name of the configuration to retrieve
        
    Returns:
    --------
    dict
        Dictionary containing configuration data
    """
    session = Session()
    
    config = session.query(Configuration).filter_by(name=name).first()
    if not config:
        session.close()
        return None
    
    # Process adjacency matrix if it exists
    adjacency_matrix = None
    if config.adjacency_matrix:
        try:
            adjacency_matrix = np.frombuffer(config.adjacency_matrix).reshape((config.n_oscillators, config.n_oscillators))
        except:
            # If there's an error reshaping, just leave as None
            pass
    
    # Create the result dictionary
    result = {
        'id': config.id,
        'name': config.name,
        'timestamp': config.timestamp,
        'n_oscillators': config.n_oscillators,
        'coupling_strength': config.coupling_strength,
        'simulation_time': config.simulation_time,
        'time_step': config.time_step,
        'random_seed': config.random_seed,
        'network_type': config.network_type,
        'frequency_distribution': config.frequency_distribution,
        'frequency_params': json.loads(config.frequency_params) if config.frequency_params else None,
        'adjacency_matrix': adjacency_matrix
    }
    
    session.close()
    return result
def export_configuration_to_json(config_id, file_path=None):
    """
    Export a saved configuration to a JSON file.
    
    Parameters:
    -----------
    config_id : int
        The ID of the configuration to export
    file_path : str, optional
        Path where the JSON file should be saved. If None, will use the config name.
        
    Returns:
    --------
    str
        Path to the exported JSON file or None if export failed
    """
    # Get the configuration
    config_dict = get_configuration(config_id)
    if not config_dict:
        return None
    
    # Determine file path if not provided
    if file_path is None:
        file_path = f"kuramoto_config_{config_dict['name'].replace(' ', '_')}.json"
    
    # Process the dictionary to make it JSON serializable
    json_serializable_dict = {}
    for key, value in config_dict.items():
        # Convert datetime objects to ISO format strings
        if isinstance(value, datetime):
            json_serializable_dict[key] = value.isoformat()
        # Convert numpy arrays to lists
        elif key == 'adjacency_matrix' and value is not None:
            # Special handling for adjacency matrix
            if hasattr(value, 'tolist'):
                adj_list = value.tolist()
                
                # Debug info about the matrix being exported
                print(f"Exporting adjacency matrix:")
                print(f"- Original shape: {value.shape}")
                print(f"- Sum of elements: {np.sum(value)}")
                print(f"- Non-zero elements: {np.count_nonzero(value)}")
                
                # Ensure we're not exporting a fully connected matrix by mistake
                # (Check if all off-diagonal elements are 1)
                n = value.shape[0]
                if np.sum(value) == n * (n - 1) and np.all(np.diag(value) == 0):
                    print("WARNING: Matrix appears to be fully connected! This might be a mistake.")
                
                json_serializable_dict[key] = adj_list
            else:
                print(f"Warning: adjacency_matrix has no tolist() method, type={type(value)}")
                json_serializable_dict[key] = value
        # Skip any non-serializable objects
        elif isinstance(value, (int, float, str, bool, list, dict)) or value is None:
            json_serializable_dict[key] = value
    
    # Add metadata
    json_serializable_dict['export_date'] = datetime.now().isoformat()
    json_serializable_dict['version'] = '1.0'
    
    # Save to file
    try:
        with open(file_path, 'w') as f:
            json.dump(json_serializable_dict, f, indent=2)
        return file_path
    except Exception as e:
        print(f"Error exporting configuration: {e}")
        return None

def import_configuration_from_json(file_path, save_to_db=True):
    """
    Import a configuration from a JSON file and optionally save it to the database.
    
    Parameters:
    -----------
    file_path : str
        Path to the JSON configuration file
    save_to_db : bool
        Whether to save the imported configuration to the database
        
    Returns:
    --------
    dict or int
        Dictionary containing configuration data, or the ID of the saved configuration if save_to_db=True
    """
    try:
        with open(file_path, 'r') as f:
            config = json.load(f)
        
        # Validate the configuration
        required_fields = ['name', 'n_oscillators', 'coupling_strength', 'simulation_time', 
                        'time_step', 'random_seed', 'network_type', 'frequency_distribution']
        
        for field in required_fields:
            if field not in config:
                print(f"Missing required field: {field}")
                return None
        
        # Convert adjacency_matrix from list to numpy array if present
        if 'adjacency_matrix' in config and config['adjacency_matrix']:
            adj_matrix = np.array(config['adjacency_matrix'])
            if adj_matrix.shape != (config['n_oscillators'], config['n_oscillators']):
                print(f"Adjacency matrix shape {adj_matrix.shape} doesn't match n_oscillators {config['n_oscillators']}")
                return None
        else:
            adj_matrix = None
        
        # Save to database if requested
        if save_to_db:
            config_id = save_configuration(
                name=config['name'],
                n_oscillators=config['n_oscillators'],
                coupling_strength=config['coupling_strength'],
                simulation_time=config['simulation_time'],
                time_step=config['time_step'],
                random_seed=config['random_seed'],
                network_type=config['network_type'],
                frequency_distribution=config['frequency_distribution'],
                frequency_params=config.get('frequency_params', {}),
                adjacency_matrix=adj_matrix
            )
            return config_id
        else:
            # Return the configuration as a dictionary
            return config
    
    except Exception as e:
        print(f"Error importing configuration: {e}")
        return None

#######################################
# Machine Learning Dataset Functions
#######################################

def create_ml_dataset(name, description=None, feature_type='time_series', target_type='regression'):
    """
    Create a new machine learning dataset that will contain multiple simulations
    and their extracted features.
    
    Parameters:
    -----------
    name : str
        Name to identify the dataset
    description : str, optional
        Description of the dataset purpose
    feature_type : str, optional
        Type of features in this dataset ('time_series', 'graph', 'spectral', etc.)
    target_type : str, optional
        Type of prediction task ('regression', 'classification', 'forecasting', etc.)
        
    Returns:
    --------
    int
        The ID of the created dataset, or None if there was an error
    """
    session = Session()
    
    # Check if dataset with this name already exists
    existing = session.query(MLDataset).filter_by(name=name).first()
    if existing:
        session.close()
        return None
    
    # Create new dataset record
    dataset = MLDataset(
        name=name,
        description=description,
        feature_type=feature_type,
        target_type=target_type,
        preprocessing=json.dumps({})  # Empty preprocessing config initially
    )
    
    session.add(dataset)
    session.commit()
    dataset_id = dataset.id
    session.close()
    
    return dataset_id


def add_simulation_to_dataset(dataset_id, simulation_id, split='train'):
    """
    Add an existing simulation to a machine learning dataset.
    
    Parameters:
    -----------
    dataset_id : int
        ID of the dataset to add to
    simulation_id : int
        ID of the simulation to add
    split : str, optional
        Which data split this simulation belongs to ('train', 'validation', 'test')
        
    Returns:
    --------
    bool
        True if successful, False otherwise
    """
    session = Session()
    
    # Check if both dataset and simulation exist
    dataset = session.query(MLDataset).filter_by(id=dataset_id).first()
    simulation = session.query(Simulation).filter_by(id=simulation_id).first()
    
    if not dataset or not simulation:
        session.close()
        return False
    
    # Check if this simulation is already in the dataset
    existing = session.query(MLDatasetSimulation).filter_by(
        dataset_id=dataset_id, simulation_id=simulation_id).first()
    
    if existing:
        # Update the split if it's changed
        if existing.split != split:
            existing.split = split
            session.commit()
        session.close()
        return True
    
    # Add the simulation to the dataset
    ds_sim = MLDatasetSimulation(
        dataset_id=dataset_id,
        simulation_id=simulation_id,
        split=split
    )
    
    session.add(ds_sim)
    session.commit()
    session.close()
    
    return True


def extract_features(dataset_id, feature_config):
    """
    Extract features from the simulations in a dataset according to a configuration.
    
    Parameters:
    -----------
    dataset_id : int
        ID of the dataset to extract features for
    feature_config : dict
        Configuration describing what features to extract, e.g.:
        {
            'order_parameter_timeseries': {
                'type': 'input',
                'description': 'Time series of order parameter r(t)',
                'extraction': 'order_parameter.r'
            },
            'final_sync_state': {
                'type': 'target',
                'description': 'Final synchronization state (r at t_max)',
                'extraction': 'order_parameter.r[-1]'
            },
            ...
        }
        
    Returns:
    --------
    bool
        True if successful, False otherwise
    """
    session = Session()
    
    # Check if dataset exists
    dataset = session.query(MLDataset).filter_by(id=dataset_id).first()
    if not dataset:
        session.close()
        return False
    
    # Get all simulations in this dataset
    ds_sims = session.query(MLDatasetSimulation).filter_by(dataset_id=dataset_id).all()
    if not ds_sims:
        session.close()
        return False
    
    # For each feature in the config, extract it from all simulations
    for feature_name, config in feature_config.items():
        # Prepare data structure to hold this feature for all simulations
        feature_data = {
            'name': feature_name,
            'simulation_ids': [],
            'values': [],
            'splits': []
        }
        
        # Extract this feature from each simulation
        for ds_sim in ds_sims:
            # Get the full simulation data
            sim_data = get_simulation(ds_sim.simulation_id)
            if not sim_data:
                continue
                
            # Extract the feature according to the extraction path
            extraction_path = config['extraction']
            feature_value = None
            
            # Handle different extraction paths
            if extraction_path == 'order_parameter.r':
                feature_value = sim_data['order_parameter']['r']
            elif extraction_path == 'order_parameter.r[-1]':
                feature_value = sim_data['order_parameter']['r'][-1]
            elif extraction_path == 'order_parameter.psi':
                feature_value = sim_data['order_parameter']['psi']
            elif extraction_path == 'phases':
                feature_value = sim_data['phases']
            elif extraction_path == 'frequencies':
                feature_value = sim_data['frequencies']
            elif extraction_path == 'adjacency_matrix':
                feature_value = sim_data['adjacency_matrix']
            elif extraction_path == 'params':
                # Extract simulation parameters as features
                feature_value = {
                    'n_oscillators': sim_data['n_oscillators'],
                    'coupling_strength': sim_data['coupling_strength'],
                    'frequency_distribution': sim_data['frequency_distribution'],
                    'frequency_params': sim_data['frequency_params']
                }
            
            if feature_value is not None:
                feature_data['simulation_ids'].append(ds_sim.simulation_id)
                feature_data['values'].append(feature_value)
                feature_data['splits'].append(ds_sim.split)
        
        # Package the feature data
        if feature_data['values']:
            # Convert to a pandas DataFrame or numpy array depending on the data
            if isinstance(feature_data['values'][0], (np.ndarray, list, tuple)):
                # For array-like data, convert to numpy array if they're all the same shape
                try:
                    feature_array = np.array(feature_data['values'])
                    feature_data_serialized = pickle.dumps(feature_array)
                except ValueError:
                    # If shapes differ, store as a list of arrays
                    feature_data_serialized = pickle.dumps(feature_data['values'])
            else:
                # For scalar or dictionary values, use pandas DataFrame
                df = pd.DataFrame({
                    'simulation_id': feature_data['simulation_ids'],
                    'value': feature_data['values'],
                    'split': feature_data['splits']
                })
                feature_data_serialized = pickle.dumps(df)
            
            # Check if this feature already exists for this dataset
            existing = session.query(MLFeature).filter_by(
                dataset_id=dataset_id, name=feature_name).first()
            
            if existing:
                # Update the existing feature
                existing.data = feature_data_serialized
                existing.description = config.get('description')
                existing.feature_type = config.get('type', 'input')
            else:
                # Create a new feature
                ml_feature = MLFeature(
                    dataset_id=dataset_id,
                    name=feature_name,
                    description=config.get('description'),
                    data=feature_data_serialized,
                    feature_type=config.get('type', 'input')
                )
                session.add(ml_feature)
    
    # Save preprocessing configuration
    dataset.preprocessing = json.dumps(feature_config)
    
    session.commit()
    session.close()
    
    return True


def get_ml_dataset(dataset_id, include_features=True):
    """
    Retrieve a machine learning dataset from the database.
    
    Parameters:
    -----------
    dataset_id : int
        ID of the dataset to retrieve
    include_features : bool, optional
        Whether to include the feature data (can be large)
        
    Returns:
    --------
    dict
        Dictionary containing dataset information and features
    """
    session = Session()
    
    # Get dataset
    dataset = session.query(MLDataset).filter_by(id=dataset_id).first()
    if not dataset:
        session.close()
        return None
    
    # Get all simulations in this dataset
    ds_sims = session.query(MLDatasetSimulation).filter_by(dataset_id=dataset_id).all()
    simulations = []
    
    for ds_sim in ds_sims:
        sim_info = session.query(Simulation).filter_by(id=ds_sim.simulation_id).first()
        if sim_info:
            simulations.append({
                'id': sim_info.id,
                'n_oscillators': sim_info.n_oscillators,
                'coupling_strength': sim_info.coupling_strength,
                'split': ds_sim.split
            })
    
    # Get features if requested
    features = []
    if include_features:
        ml_features = session.query(MLFeature).filter_by(dataset_id=dataset_id).all()
        for feature in ml_features:
            feature_info = {
                'id': feature.id,
                'name': feature.name,
                'description': feature.description,
                'type': feature.feature_type
            }
            
            # Only include actual data if specifically requested (can be large)
            if include_features:
                try:
                    feature_info['data'] = pickle.loads(feature.data)
                except:
                    feature_info['data'] = None
                    
            features.append(feature_info)
    
    # Create the result dictionary
    result = {
        'id': dataset.id,
        'name': dataset.name,
        'description': dataset.description,
        'feature_type': dataset.feature_type,
        'target_type': dataset.target_type,
        'preprocessing': json.loads(dataset.preprocessing) if dataset.preprocessing else {},
        'simulations': simulations,
        'features': features
    }
    
    session.close()
    return result


def list_ml_datasets():
    """
    List all machine learning datasets in the database.
    
    Returns:
    --------
    list
        List of dictionaries containing basic dataset info
    """
    session = Session()
    
    datasets = session.query(MLDataset).all()
    result = []
    
    for ds in datasets:
        # Count simulations by split
        sim_counts = {}
        ds_sims = session.query(MLDatasetSimulation).filter_by(dataset_id=ds.id).all()
        for split in ['train', 'validation', 'test']:
            sim_counts[split] = len([s for s in ds_sims if s.split == split])
        
        # Count features by type
        feature_counts = {}
        features = session.query(MLFeature).filter_by(dataset_id=ds.id).all()
        for f_type in ['input', 'target', 'metadata']:
            feature_counts[f_type] = len([f for f in features if f.feature_type == f_type])
        
        result.append({
            'id': ds.id,
            'name': ds.name,
            'description': ds.description,
            'feature_type': ds.feature_type,
            'target_type': ds.target_type,
            'simulation_counts': sim_counts,
            'feature_counts': feature_counts
        })
    
    session.close()
    return result


def delete_ml_dataset(dataset_id):
    """
    Delete a machine learning dataset from the database.
    
    Parameters:
    -----------
    dataset_id : int
        ID of the dataset to delete
        
    Returns:
    --------
    bool
        True if successful, False otherwise
    """
    session = Session()
    
    dataset = session.query(MLDataset).filter_by(id=dataset_id).first()
    if not dataset:
        session.close()
        return False
    
    session.delete(dataset)
    session.commit()
    session.close()
    
    return True


def export_ml_dataset(dataset_id, file_path=None, format='numpy'):
    """
    Export a machine learning dataset to files that can be used for training models.
    
    Parameters:
    -----------
    dataset_id : int
        ID of the dataset to export
    file_path : str, optional
        Directory where files should be saved. If None, will use the dataset name.
    format : str, optional
        Export format: 'numpy', 'pandas', 'pytorch', or 'tensorflow'
        
    Returns:
    --------
    str
        Path to the exported files, or None if export failed
    """
    # Get the dataset
    dataset = get_ml_dataset(dataset_id)
    if not dataset:
        return None
    
    # Create directory if it doesn't exist
    if file_path is None:
        file_path = f"./ml_datasets/{dataset['name'].replace(' ', '_')}"
    
    os.makedirs(file_path, exist_ok=True)
    
    # Organize features by type and split
    features_by_type = {'input': [], 'target': [], 'metadata': []}
    for feature in dataset['features']:
        features_by_type[feature['type']].append(feature)
    
    data_by_split = {'train': {}, 'validation': {}, 'test': {}}
    
    # Process each feature by its type and organize by split
    for feature_type, features in features_by_type.items():
        for feature in features:
            if isinstance(feature['data'], pd.DataFrame):
                # For DataFrame features, separate by split
                for split in ['train', 'validation', 'test']:
                    split_data = feature['data'][feature['data']['split'] == split]
                    if not split_data.empty:
                        if feature['name'] not in data_by_split[split]:
                            data_by_split[split][feature['name']] = {}
                        data_by_split[split][feature['name']] = split_data['value'].values
            elif isinstance(feature['data'], np.ndarray):
                # For numpy array features, use the simulation_ids to determine splits
                # This is more complex and requires tracking which index goes with which simulation
                # For simplicity here, we'll store the whole array
                for split in ['train', 'validation', 'test']:
                    if feature['name'] not in data_by_split[split]:
                        data_by_split[split][feature['name']] = {}
                    data_by_split[split][feature['name']] = feature['data']
    
    # Save the data according to the requested format
    if format == 'numpy':
        for split, data in data_by_split.items():
            if data:
                split_dir = os.path.join(file_path, split)
                os.makedirs(split_dir, exist_ok=True)
                for feature_name, feature_data in data.items():
                    np.save(os.path.join(split_dir, f"{feature_name}.npy"), feature_data)
    
    elif format == 'pandas':
        for split, data in data_by_split.items():
            if data:
                split_dir = os.path.join(file_path, split)
                os.makedirs(split_dir, exist_ok=True)
                # Convert to DataFrame and save as CSV
                df = pd.DataFrame(data)
                df.to_csv(os.path.join(split_dir, f"{split}_data.csv"), index=False)
    
    # Save metadata about the dataset
    metadata = {
        'name': dataset['name'],
        'description': dataset['description'],
        'feature_type': dataset['feature_type'],
        'target_type': dataset['target_type'],
        'preprocessing': dataset['preprocessing'],
        'features': [{'name': f['name'], 'type': f['type'], 'description': f['description']} 
                    for f in dataset['features']],
        'export_format': format,
        'export_time': datetime.now().isoformat(),
        'splits': {split: len(data) for split, data in data_by_split.items()}
    }
    
    with open(os.path.join(file_path, 'metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    return file_path


def run_batch_simulations(config_variations, base_config=None, dataset_name=None):
    """
    Run a batch of simulations with varying parameters and optionally add them to a dataset.
    
    Parameters:
    -----------
    config_variations : list of dict
        List of configuration variations to simulate, each a dict of parameter overrides
    base_config : dict, optional
        Base configuration to use for all simulations (will be overridden by variations)
    dataset_name : str, optional
        If provided, will create a new ML dataset and add all simulations to it
        
    Returns:
    --------
    dict
        Results of the batch run with simulation IDs and dataset ID if created
    """
    # This function requires importing the KuramotoModel class
    from kuramoto_model import KuramotoModel
    
    # Create a dataset if requested
    dataset_id = None
    if dataset_name:
        dataset_id = create_ml_dataset(
            name=dataset_name,
            description=f"Batch simulation run with {len(config_variations)} variations",
            feature_type='time_series',
            target_type='regression'
        )
    
    # Set up default base configuration if none provided
    if base_config is None:
        base_config = {
            'n_oscillators': 10,
            'coupling_strength': 1.0,
            'simulation_time': 10.0,
            'time_step': 0.01,
            'random_seed': 42,
            'network_type': 'all-to-all',
            'frequency_distribution': 'normal',
            'frequency_params': {'mean': 0.0, 'std': 0.1}
        }
    
    # Run all simulations
    simulation_results = []
    
    for i, variation in enumerate(config_variations):
        # Create a copy of the base config and apply variations
        config = base_config.copy()
        config.update(variation)
        
        # Determine data split (for ML dataset)
        # Simple strategy: 70% train, 15% validation, 15% test
        if i < int(len(config_variations) * 0.7):
            split = 'train'
        elif i < int(len(config_variations) * 0.85):
            split = 'validation'
        else:
            split = 'test'
        
        # Set up the model
        model = KuramotoModel(
            n_oscillators=config['n_oscillators'],
            coupling_strength=config['coupling_strength'],
            simulation_time=config['simulation_time'],
            time_step=config['time_step'],
            random_seed=config['random_seed']
        )
        
        # Generate frequencies according to distribution
        if config['frequency_distribution'] == 'normal':
            mean = config['frequency_params'].get('mean', 0.0)
            std = config['frequency_params'].get('std', 0.1)
            np.random.seed(config['random_seed'])
            frequencies = np.random.normal(mean, std, config['n_oscillators'])
        elif config['frequency_distribution'] == 'uniform':
            min_val = config['frequency_params'].get('min', -0.5)
            max_val = config['frequency_params'].get('max', 0.5)
            np.random.seed(config['random_seed'])
            frequencies = np.random.uniform(min_val, max_val, config['n_oscillators'])
        elif config['frequency_distribution'] == 'bimodal':
            peak1 = config['frequency_params'].get('peak1', -0.5)
            peak2 = config['frequency_params'].get('peak2', 0.5)
            std = config['frequency_params'].get('std', 0.1)
            np.random.seed(config['random_seed'])
            frequencies = np.concatenate([
                np.random.normal(peak1, std, config['n_oscillators'] // 2),
                np.random.normal(peak2, std, config['n_oscillators'] // 2 + config['n_oscillators'] % 2)
            ])
        else:
            # Default to normal distribution
            np.random.seed(config['random_seed'])
            frequencies = np.random.normal(0.0, 0.1, config['n_oscillators'])
        
        # Generate network topology
        adjacency_matrix = None
        if config['network_type'] == 'all-to-all':
            # Fully connected network (excluding self-connections)
            adjacency_matrix = np.ones((config['n_oscillators'], config['n_oscillators'])) - np.eye(config['n_oscillators'])
        elif config['network_type'] == 'ring':
            # Ring network (each oscillator connected to its neighbors)
            adjacency_matrix = np.zeros((config['n_oscillators'], config['n_oscillators']))
            for i in range(config['n_oscillators']):
                adjacency_matrix[i, (i-1) % config['n_oscillators']] = 1
                adjacency_matrix[i, (i+1) % config['n_oscillators']] = 1
        elif config['network_type'] == 'random':
            # Random network with 30% connection probability
            np.random.seed(config['random_seed'])
            adjacency_matrix = (np.random.random((config['n_oscillators'], config['n_oscillators'])) < 0.3).astype(float)
            # Remove self-connections
            np.fill_diagonal(adjacency_matrix, 0)
        
        # Run the simulation
        times, phases, order_parameter = model.simulate()
        
        # Store the simulation
        sim_id = store_simulation(
            model=model,
            times=times,
            phases=phases,
            order_parameter=order_parameter,
            frequencies=frequencies,
            freq_type=config['frequency_distribution'],
            freq_params=config['frequency_params'],
            adjacency_matrix=adjacency_matrix
        )
        
        # Add to dataset if one was created
        if dataset_id:
            add_simulation_to_dataset(dataset_id, sim_id, split=split)
        
        # Record the result
        simulation_results.append({
            'id': sim_id,
            'config': config,
            'split': split
        })
    
    # Return the batch results
    return {
        'dataset_id': dataset_id,
        'simulations': simulation_results,
        'total_simulations': len(simulation_results)
    }
        
        # Convert lists back to numpy arrays if needed
        if config_dict.get('adjacency_matrix') is not None:
            try:
                # Ensure adjacency matrix is a numpy array
                adj_matrix = np.array(config_dict['adjacency_matrix'])
                
                # Make sure no self-loops (diagonal should be zero)
                np.fill_diagonal(adj_matrix, 0)
                
                print(f"Converted adjacency matrix to numpy array:")
                print(f"- Shape: {adj_matrix.shape}")
                print(f"- Sum: {np.sum(adj_matrix)}")
                print(f"- Non-zero elements: {np.count_nonzero(adj_matrix)}")
                
                # Determine if it's a fully connected matrix (all 1s except diagonal)
                is_fully_connected = np.all(
                    (adj_matrix == 1) | 
                    (np.eye(adj_matrix.shape[0]) == 1)
                )
                
                if is_fully_connected:
                    print("WARNING: Matrix appears to be fully connected. Checking if this is correct...")
                    # If every off-diagonal element is 1, it might be mistakenly set to fully connected
                    if np.sum(adj_matrix) == adj_matrix.shape[0] * (adj_matrix.shape[0] - 1):
                        print("This seems to be a 'All-to-All' network type rather than a custom matrix.")
                
                config_dict['adjacency_matrix'] = adj_matrix
                
                # Verify network type is set correctly
                if config_dict.get('network_type') != "Custom Adjacency Matrix":
                    print(f"Updating network type from {config_dict.get('network_type')} to 'Custom Adjacency Matrix'")
                    config_dict['network_type'] = "Custom Adjacency Matrix"
            except Exception as e:
                print(f"Error processing adjacency matrix: {e}")
                config_dict['adjacency_matrix'] = None
        
        # Save to database if requested
        if save_to_db:
            # Generate a unique name if needed
            name = config_dict.get('name', os.path.basename(file_path).split('.')[0])
            session = Session()
            existing = session.query(Configuration).filter_by(name=name).first()
            session.close()
            
            if existing:
                name = f"{name}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
            
            # Save to database - prepare frequency_params
            freq_params = config_dict.get('frequency_params')
            if isinstance(freq_params, str):
                try:
                    freq_params = json.loads(freq_params)
                except:
                    pass  # Keep as string if JSON parsing fails
            
            # Handle adjacency matrix
            adj_matrix = config_dict.get('adjacency_matrix')
            
            # Call the save_configuration function
            config_id = save_configuration(
                name=name,
                n_oscillators=config_dict.get('n_oscillators'),
                coupling_strength=config_dict.get('coupling_strength'),
                simulation_time=config_dict.get('simulation_time'),
                time_step=config_dict.get('time_step'),
                random_seed=config_dict.get('random_seed'),
                network_type=config_dict.get('network_type'),
                frequency_distribution=config_dict.get('frequency_distribution'),
                frequency_params=freq_params,
                adjacency_matrix=adj_matrix
            )
            
            return config_id
        
        return config_dict
    
    except Exception as e:
        print(f"Error importing configuration: {e}")
        return None

